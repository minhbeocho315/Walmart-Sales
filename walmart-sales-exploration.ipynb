{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8620416,"sourceType":"datasetVersion","datasetId":4438189}],"dockerImageVersionId":30646,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <div style=\"background: #6b8272;font-family: monospace; font-weight: bold; font-size: 110%; text-align: center; border-bottom: 0.3px solid #004466;\"> Walmart Sales Analysis </div>\n    \n\nWalmart Inc. is an American multinational retail corporation that operates a chain of hypermarkets, discount department stores, and grocery stores in the United States, headquartered in Bentonville, Arkansas. \n\n![](https://corporate.walmart.com/content/corporate/en_us/about/jcr:content/par/grid_4_copy_copy/parsys_tab_1/image.img.jpg/1693432306522.jpg)","metadata":{}},{"cell_type":"markdown","source":"# <div style=\"background: #6b8272; font-family: monospace; font-weight: bold; font-size: 110%; text-align: center; border-bottom: 0.3px solid #004466;\"> PROJECT OVERVIEW </div>\n    \n* [INTRODUCTION](#1)\n* [CONFIGURATION](#2)\n    * [LIBRARY DEPENDENCIES](#2.1)\n    * [DATA SOURCE & PREPARATION](#2.2)    \n* [PREPROCESSING](#3)\n    * [DATA CLEANING](#3.1)\n    * [DATA EXPLORATION](#3.2)\n* [FEATURE ENGINEERING](#4)\n* [EXPLORATORY DATA ANALYSIS](#5)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n# <div style=\"background: #6b8272;font-family: monospace; font-weight: bold; font-size: 110%; text-align: center; border-bottom: 0.3px solid #004466;\"> INTRODUCTION </div>\n\n\nThis project conducts an in-depth analysis of retail sales data, specifically focusing on Walmart store performance across different locations. Using Python for data exploration and visualization, I aim to uncover insights into sales trends, seasonal variations, and the influence of external factors like holidays, temperature, fuel prices, CPI, and unemployment rates. Through this analysis, I aim to provide actionable insights for retail management decision-making and enhance understanding of consumer behavior and market dynamics.\n<br><br>   \n\n<b>Summary of the Dataset:</b>\n1. Store: Identifier for the retail store.\n2. Date: Date of sales record.\n3. Holiday_Flag: Indicator for holiday week (1) or non-holiday week (0).\n4. Temperature: Temperature in the region of the store.\n5. Fuel_Price: Fuel price in the region.\n6. CPI: Consumer Price Index.\n7. Unemployment: Unemployment rate.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n## <div style=\"background: #6b8272;font-family: monospace; font-weight: bold; font-size: 110%; text-align: center; border-bottom: 0.3px solid #004466;\"> CONFIGURATION </div>\n    \n* <b>Libraries: </b>The analysis utilizes Python libraries such as Pandas, Matplotlib, scipy, calendar, and Seaborn for data manipulation and visualization.\n* <b>Dataset:</b> The dataset used in this analysis includes variables such as Store, Date, Weekly_Sales, Holiday_Flag, Temperature, Fuel_Price, CPI, and Unemployment, providing a comprehensive view of Walmart sales data.\n\n\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"2.1\"></a>\n# <div style=\"background: #6b8272;font-family: monospace; font-weight: bold; font-size: 70%; text-align: center; border-bottom: 0.3px solid #004466;\"> Essential Library Imports for Data Analysis </div>","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Installing select libraries:-\nfrom gc import collect; # garbage collection to free up memory\nfrom warnings import filterwarnings; # handle warning messages\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\n\nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns # statistical data visualization\n\n# Set the plot style to 'fivethirtyeight'\nplt.style.use(\"fivethirtyeight\")\n\nfrom datetime import datetime  # Importing the datetime class from the datetime module\n\nfrom scipy import stats # statistical functions\n\nfilterwarnings('ignore'); # Ignore warning messages\nfrom IPython.display import display_html, clear_output; # displaying HTML content\n\n\nclear_output();\nprint();\ncollect();","metadata":{"execution":{"iopub.status.busy":"2024-02-28T15:00:36.21757Z","iopub.execute_input":"2024-02-28T15:00:36.218097Z","iopub.status.idle":"2024-02-28T15:00:36.380148Z","shell.execute_reply.started":"2024-02-28T15:00:36.218051Z","shell.execute_reply":"2024-02-28T15:00:36.378754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"2.2\"></a>\n# <div style=\"background: #6b8272;font-family: monospace; font-weight: bold; font-size: 70%; text-align: center; border-bottom: 0.3px solid #004466;\"> Loading and Preparing the Dataset </div>","metadata":{}},{"cell_type":"code","source":"%%time\n\n# Error Handling When Loading Dataset with Pandas read_csv\n\ntry:\n    # Attempt to read the dataset\n    df = pd.read_csv('/kaggle/input/walmart-sales/Walmart_sales.csv')\n    print(\"Dataset loaded successfully.\")\n    \nexcept FileNotFoundError:\n    # Handle FileNotFoundError if the file does not exist\n    print(\"Error: File not found. Please check the file path.\")\n\nexcept Exception as e:\n    # Handle other exceptions\n    print(\"An error occurred while loading the dataset:\", e)\n\nprint();\ncollect();","metadata":{"execution":{"iopub.status.busy":"2024-02-28T15:00:36.383016Z","iopub.execute_input":"2024-02-28T15:00:36.383641Z","iopub.status.idle":"2024-02-28T15:00:36.574459Z","shell.execute_reply.started":"2024-02-28T15:00:36.383594Z","shell.execute_reply":"2024-02-28T15:00:36.573028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3\"></a>\n# <div style=\"background: #6b8272;font-family: monospace; font-weight: bold; font-size: 110%; text-align: center; border-bottom: 0.3px solid #004466;\"> DATA PREPROCESSING </div>\n    \n1. <b>Data Cleaning: </b>\n* Handling Missing Values\n* Duplicate Values\n* Data Type Conversations\n\nThis stage involves preparing the dataset for analysis by addressing missing values and ensuring uniform data types.\n\n2. <b>Data Exploration:</b>\n* Descriptive Statistics\n* Dataset Shape Analysis\n\nThis phase focuses on exploring the dataset's characteristics through statistical summaries and understanding its structure and dimensions.\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.1\"></a>\n## <div style=\"background: #6b8272;font-family: monospace; font-weight: bold; font-size: 70%; text-align: center; border-bottom: 0.3px solid #004466;\"> Data Cleaning ( Column formating, Missing values, duplicate values, data type ) </div>","metadata":{}},{"cell_type":"code","source":"# check the columns name\ndf.columns","metadata":{"execution":{"iopub.status.busy":"2024-02-28T15:00:36.576613Z","iopub.execute_input":"2024-02-28T15:00:36.577076Z","iopub.status.idle":"2024-02-28T15:00:36.587313Z","shell.execute_reply.started":"2024-02-28T15:00:36.577021Z","shell.execute_reply":"2024-02-28T15:00:36.585736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\n# Rename columns to lowercase\ndf.columns = df.columns.str.lower()\n\n# Verify the new column names\nprint(\"\\nNew column names:\")\nprint(df.columns)\n\nprint();\ncollect();","metadata":{"execution":{"iopub.status.busy":"2024-02-28T15:00:36.589414Z","iopub.execute_input":"2024-02-28T15:00:36.589938Z","iopub.status.idle":"2024-02-28T15:00:36.759001Z","shell.execute_reply.started":"2024-02-28T15:00:36.589896Z","shell.execute_reply":"2024-02-28T15:00:36.757717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the null values in the dataset\ndf.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-02-28T15:00:36.762888Z","iopub.execute_input":"2024-02-28T15:00:36.763335Z","iopub.status.idle":"2024-02-28T15:00:36.781045Z","shell.execute_reply.started":"2024-02-28T15:00:36.763299Z","shell.execute_reply":"2024-02-28T15:00:36.779569Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#information about the dataset\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2024-02-28T15:00:36.782957Z","iopub.execute_input":"2024-02-28T15:00:36.783353Z","iopub.status.idle":"2024-02-28T15:00:36.799779Z","shell.execute_reply.started":"2024-02-28T15:00:36.78332Z","shell.execute_reply":"2024-02-28T15:00:36.797818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Attempt to convert 'date' column to datetime format\ntry:\n    df['date'] = pd.to_datetime(df['date'], format='mixed')\n    print(\"All values in 'date' column are valid dates.\")\nexcept ValueError as e:\n    print(\"Error:\", e)\n    print(\"There are non-date values present in the 'date' column.\")","metadata":{"execution":{"iopub.status.busy":"2024-02-28T15:00:36.80188Z","iopub.execute_input":"2024-02-28T15:00:36.802249Z","iopub.status.idle":"2024-02-28T15:00:36.813491Z","shell.execute_reply.started":"2024-02-28T15:00:36.802217Z","shell.execute_reply":"2024-02-28T15:00:36.812145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the duplicate values in the data\nduplicate_values=df.duplicated().sum()\nprint(f'The data contains {duplicate_values} duplicate values')","metadata":{"execution":{"iopub.status.busy":"2024-02-28T15:00:36.815574Z","iopub.execute_input":"2024-02-28T15:00:36.815957Z","iopub.status.idle":"2024-02-28T15:00:36.82853Z","shell.execute_reply.started":"2024-02-28T15:00:36.815925Z","shell.execute_reply":"2024-02-28T15:00:36.827312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"3.2\"></a>\n# <div style=\"background: #6b8272;font-family: monospace; font-weight: bold; font-size: 70%; text-align: center; border-bottom: 0.3px solid #004466;\"> Data Exploration </div>","metadata":{}},{"cell_type":"markdown","source":"<a id=\"3.2\"></a>\n","metadata":{}},{"cell_type":"code","source":"# Checking the data shape\nprint(f'The dataset contains {df.shape[0]} rows and {df.shape[1]} columns')","metadata":{"execution":{"iopub.status.busy":"2024-02-28T15:00:36.830621Z","iopub.execute_input":"2024-02-28T15:00:36.831122Z","iopub.status.idle":"2024-02-28T15:00:36.840348Z","shell.execute_reply.started":"2024-02-28T15:00:36.831081Z","shell.execute_reply":"2024-02-28T15:00:36.839413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Statistics about the data set\ndf.describe().style.background_gradient(cmap='bone_r')","metadata":{"execution":{"iopub.status.busy":"2024-02-28T15:00:36.842078Z","iopub.execute_input":"2024-02-28T15:00:36.842502Z","iopub.status.idle":"2024-02-28T15:00:36.899458Z","shell.execute_reply.started":"2024-02-28T15:00:36.842448Z","shell.execute_reply":"2024-02-28T15:00:36.898558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"4\"></a>\n# <div style=\"background: #6b8272;font-family: monospace; font-weight: bold; font-size: 110%; text-align: center; border-bottom: 0.3px solid #004466;\"> FEATURE ENGINEERING </div>\n    \n* <b>Date Features: </b>Extracting additional information from the 'Date' column such as day, month, year, day of the week, and whether it's a holiday or not. This can provide seasonality information and holiday effects.\n\nList of Numerical Features\n* weekly sales\n* temperature\n* fuel price\n* cpi\n* unemployment","metadata":{}},{"cell_type":"code","source":"# Function to map dates to seasons\ndef date_to_season(date):\n    # Extract month from date string and convert it to integer\n    month = datetime.strptime(date, \"%Y-%m-%d\").month\n    \n    # Map months to seasons\n    if month in [3, 4, 5]:\n        return \"Spring\"\n    elif month in [6, 7, 8]:\n        return \"Summer\"\n    elif month in [9, 10, 11]:\n        return \"Autumn\"\n    else:\n        return \"Winter\"\n    \n# Apply date_to_season function to create a new column\ndf[\"season\"] = df[\"date\"].apply(lambda x: date_to_season(x.strftime(\"%Y-%m-%d\")))","metadata":{"execution":{"iopub.status.busy":"2024-02-28T15:00:36.900957Z","iopub.execute_input":"2024-02-28T15:00:36.901867Z","iopub.status.idle":"2024-02-28T15:00:37.066756Z","shell.execute_reply.started":"2024-02-28T15:00:36.901832Z","shell.execute_reply":"2024-02-28T15:00:37.06533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract year from the 'date' column\ndf['year'] = df['date'].dt.year\n\n# Extract month from the 'date' column\ndf['month'] = df['date'].dt.month\n\n# Extract month name from the 'date' column\ndf['month_name'] = df['date'].dt.month_name()\n\n# Extract day from the 'date' column\ndf['day'] = df['date'].dt.day\n\n# Extract day of the week (0 = Monday, 1 = Tuesday, ..., 6 = Sunday) from the 'date' column\ndf['day_of_week'] = df['date'].dt.dayofweek\n\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2024-02-28T15:00:37.068835Z","iopub.execute_input":"2024-02-28T15:00:37.069257Z","iopub.status.idle":"2024-02-28T15:00:37.104751Z","shell.execute_reply.started":"2024-02-28T15:00:37.069223Z","shell.execute_reply":"2024-02-28T15:00:37.103445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"5\"></a>\n# <div style=\"background: #6b8272;font-family: monospace; font-weight: bold; font-size: 110%; text-align: center; border-bottom: 0.3px solid #004466;\"> EXPLORATORY DATA ANALYSIS(EDA) </div>\n    \nUnivariate Analysis:\n\n* Visualize the distribution of numerical variables using histograms or density plots.\n* Explore categorical variables using bar plots or count plots to understand their frequency distribution.\n\nBivariate Analysis:\n\n* Analyze relationships between pairs of variables using scatter plots (for numerical variables) or grouped bar plots (for categorical variables).\n* Calculate correlation coefficients between numerical variables to measure the strength and direction of linear relationships.\n\n\nVisualizing distributions of variables (histograms, kernel density plots)\nSummary statistics (box plots, violin plots)\nPairwise relationships (scatter plots, pair plots)","metadata":{}},{"cell_type":"code","source":"# List of features\nfeatures = ['weekly_sales', 'temperature', 'fuel_price', 'cpi', 'unemployment']\n\n# Set the figure size\nplt.figure(figsize=(18, 20))\n\n# Loop through each column in your dataset\nfor i, col in enumerate(features):\n    # Create subplots\n    plt.subplot(3, 3, i+1)\n    \n    # Plot histogram for the current column\n    sns.histplot(data=df, x=col, kde=True)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-28T15:00:37.106617Z","iopub.execute_input":"2024-02-28T15:00:37.106978Z","iopub.status.idle":"2024-02-28T15:00:39.198971Z","shell.execute_reply.started":"2024-02-28T15:00:37.106946Z","shell.execute_reply":"2024-02-28T15:00:39.197762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background: #4d4d00; padding: 20px; font-family: monospace; font-weight: bold; font-size: 90%; text-align: left; width: 100%; box-sizing: border-box; color: white;\">\n    <u><strong>Conclusion</strong></u><br>\n    <br>\n    <b>Weekly Sales:</b> The histogram of weekly sales indicates a right-skewed distribution, suggesting that there are relatively fewer instances of very high sales compared to lower sales amounts. This could indicate occasional spikes in sales or a few high-performing periods.<br>\n    <br>\n    <b>Temperature and Unemployment:</b> The histograms of temperature and unemployment show approximately normal distributions, indicating that the majority of the data points are centered around the mean with relatively few outliers. This suggests that these factors may follow typical patterns without significant deviations.<br>\n    <br>\n    <b>Fuel Price, CPI:</b> The histograms of fuel price and CPI exhibit bimodal distributions, suggesting the presence of two distinct peaks or modes in the data. This could imply the existence of different market conditions or states within the dataset, potentially indicating varying economic situations or consumer behaviors.\n</div>\n","metadata":{}},{"cell_type":"code","source":"correlation_matrix = df[['weekly_sales', 'temperature', 'fuel_price', 'cpi', 'unemployment']].corr()\nplt.figure(figsize=(8, 6))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Heatmap')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-28T15:00:39.204447Z","iopub.execute_input":"2024-02-28T15:00:39.204931Z","iopub.status.idle":"2024-02-28T15:00:39.608951Z","shell.execute_reply.started":"2024-02-28T15:00:39.204885Z","shell.execute_reply":"2024-02-28T15:00:39.607546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background: #4d4d00; padding: 20px; font-family: monospace; font-weight: bold; font-size: 90%; text-align: left; width: 100%; box-sizing: border-box; color: white;\">\n    <u><strong>Conclusion</strong></u><br>\n    <ul>\n        <li>Weekly sales have a weak negative correlation with temperature, fuel price, and unemployment. This means that as these variables increase, weekly sales tend to decrease slightly.</li>\n        <li>Weekly sales have a weak positive correlation with CPI. This means that as CPI increases, weekly sales tend to increase slightly.</li>\n        <li>Temperature has a weak positive correlation with fuel price, CPI, and unemployment.</li>\n        <li>Fuel price has a weak negative correlation with CPI.</li>\n        <li>CPI has a moderate negative correlation with unemployment.</li>\n    </ul>\n</div>\n","metadata":{}},{"cell_type":"code","source":"pd.pivot_table(data = df,\n              index = 'year',\n              columns = 'season',\n              values = 'weekly_sales',\n              aggfunc = 'sum')","metadata":{"execution":{"iopub.status.busy":"2024-02-28T15:00:39.610403Z","iopub.execute_input":"2024-02-28T15:00:39.61077Z","iopub.status.idle":"2024-02-28T15:00:39.638124Z","shell.execute_reply.started":"2024-02-28T15:00:39.610739Z","shell.execute_reply":"2024-02-28T15:00:39.636928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a dictionary to store season-wise weekly sales for each year\nseasonwise_weekly_sales = {}\n\n# Iterate over unique seasons\nfor season in df['season'].unique():\n    # Group by year and sum the weekly sales for the current season\n    season_sales = df[df['season'] == season].groupby('year')['weekly_sales'].sum()\n    # Store the season-wise weekly sales in the dictionary\n    seasonwise_weekly_sales[season] = season_sales\n\n# Create an empty list to store data\nplot_data = []\n\n# Populate the list with data\nfor season, sales in seasonwise_weekly_sales.items():\n    for year, weekly_sales in sales.items():\n        plot_data.append({'Year': year, 'Season': season, 'Weekly Sales': weekly_sales})\n\n# Convert the list to a DataFrame\nplot_data = pd.DataFrame(plot_data)\n\n# Create the figure and axes\nfig, ax = plt.subplots(figsize=(10, 6))\n\n# Plot the stacked bar plot using seaborn\nsns.barplot(data=plot_data, x='Year', y='Weekly Sales', hue='Season', ax=ax, ci=None)\n\n# Set labels and title\nax.set_xlabel('Year')\nax.set_ylabel('Total Weekly Sales')\nax.set_title('Yearly Season-wise Total Weekly Sales')\n# Adjust legend position to prevent it from going outside the plot\nax.legend(title='Season', loc='upper left', bbox_to_anchor=(1, 1))\n\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-28T15:00:39.64014Z","iopub.execute_input":"2024-02-28T15:00:39.640896Z","iopub.status.idle":"2024-02-28T15:00:40.018119Z","shell.execute_reply.started":"2024-02-28T15:00:39.640863Z","shell.execute_reply":"2024-02-28T15:00:40.016786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background: #4d4d00; padding: 20px; font-family: monospace; font-weight: bold; font-size: 90%; text-align: left; width: 100%; box-sizing: border-box; color: white;\">\n    <u><strong>Conclusion: Best Season by Year</strong></u><br>\n    <ul>\n        <li><strong>2010:</strong> Spring</li>\n        <li><strong>2011:</strong> Autumn</li>\n        <li><strong>2012:</strong> Summer</li>\n    </ul>\n</div>","metadata":{}},{"cell_type":"code","source":"# Group by year and holiday_flag to get counts\nholiday_counts = df.groupby(['year', 'holiday_flag']).size().unstack(fill_value=0).reset_index()\n\n# Melt DataFrame to long format\nholiday_counts_melted = pd.melt(holiday_counts, id_vars='year', var_name='Holiday Flag', value_name='Count')\n\n# Plot using Seaborn\nfig, ax = plt.subplots(1, 2, figsize=(12, 6))\n\n# Bar plot\nsns.barplot(data=holiday_counts_melted, x='year', y='Count', hue='Holiday Flag', ax=ax[0])\nax[0].set_title('Holiday Distribution Over the Years')\nax[0].set_xlabel('Year')\nax[0].set_ylabel('Count')\n\n# Get legend handles\nhandles, _ = ax[0].get_legend_handles_labels()\n\nax[0].legend(handles=handles, labels=['Not Holiday', 'Holiday'], title='Holiday Flag', loc='upper left', bbox_to_anchor=(1, 1))\n\nax[1].pie(df['holiday_flag'].value_counts().values, labels=['Not Holiday', 'Holiday'], autopct='%1.2f%%')\nax[1].set_title('Overall Holiday Distribution')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-28T15:00:40.019315Z","iopub.execute_input":"2024-02-28T15:00:40.020313Z","iopub.status.idle":"2024-02-28T15:00:40.421605Z","shell.execute_reply.started":"2024-02-28T15:00:40.020256Z","shell.execute_reply":"2024-02-28T15:00:40.420402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background: #4d4d00; padding: 20px; font-family: monospace; font-weight: bold; font-size: 90%; text-align: left; width: 100%; box-sizing: border-box; color: white;\">\n    <u><strong>Conclusion</strong></u><br>\n    Based on the analysis of holiday distribution over the years, it appears to be a typical scenario, with holiday percentages varying across different years. The overall holiday percentage, calculated as 6.99%, indicates that holidays occur in a relatively small proportion of the observed data points. Further investigation into the specific trends and patterns within each year may provide additional insights into holiday occurrences and their impact.\n</div>","metadata":{}},{"cell_type":"code","source":"# Calculate the count of each year\nyear_counts = df['year'].value_counts()\n\n# Plotting\nfig, ax = plt.subplots(1, 2, figsize=(14, 6))\n\n# Countplot for the distribution of years\nsns.countplot(data=df, x='year', ax=ax[0])\nax[0].set_xlabel('Year')\nax[0].set_ylabel('Count')\n\n# Pie chart for the distribution of years\nax[1].pie(year_counts.values, labels=year_counts.index, autopct='%1.2f%%')\n\n# Set a single title for the entire figure\nplt.suptitle('Distribution of Years', fontsize=16, y=1.05)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-28T15:00:40.423076Z","iopub.execute_input":"2024-02-28T15:00:40.423422Z","iopub.status.idle":"2024-02-28T15:00:40.728797Z","shell.execute_reply.started":"2024-02-28T15:00:40.423383Z","shell.execute_reply":"2024-02-28T15:00:40.727596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Due assignment\n* More Visualization\n* Machine Learning Model\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}